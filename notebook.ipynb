{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882af774",
   "metadata": {},
   "source": [
    "# ðŸ§ª Toxic Comment Classification (Jigsaw) â€” End-to-End Notebook\n",
    "# Single-model optimization with a quick 2-model benchmark phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a43e2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Setup & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fa26986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.57.1\n",
      "Torch: 2.5.1\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, json, random, gc, time, shutil, inspect\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import transformers as hf\n",
    "\n",
    "print(\"Transformers:\", hf.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED   = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ====== PATHS: set this to your local location ======\n",
    "DATA_PATH = \"./data/train.csv\"  # <- change if needed\n",
    "OUT_DIR   = \"./outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Labels in Jigsaw dataset\n",
    "LABELS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "\n",
    "# Base configs\n",
    "BASE_MODELS = {\n",
    "    \"distilbert-base-uncased\": {\"max_length\": 192},\n",
    "    \"bert-base-uncased\":      {\"max_length\": 192}\n",
    "}\n",
    "\n",
    "# Subset sizes (smaller on CPU to keep it practical)\n",
    "if DEVICE == \"cuda\":\n",
    "    QUICK_TRAIN_SIZE = 8000   # for quick benchmark per model\n",
    "    QUICK_VAL_SIZE   = 2000\n",
    "else:\n",
    "    QUICK_TRAIN_SIZE = 2000\n",
    "    QUICK_VAL_SIZE   = 800\n",
    "\n",
    "# Final training sizes (Phase 2) â€” you can increase if GPU available\n",
    "FINAL_TRAIN_FRACTION = 0.9   # 90% of train split used for training; rest for val\n",
    "EPOCHS_BENCHMARK     = 1     # quick head-to-head\n",
    "EPOCHS_FINAL         = 3     # main optimization run\n",
    "BATCH_SIZE           = 16 if DEVICE == \"cuda\" else 8\n",
    "LR_BENCHMARK         = 2e-5\n",
    "LR_FINAL             = 2e-5\n",
    "WEIGHT_DECAY         = 0.01\n",
    "PATIENCE             = 2     # early stopping patience for final run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d6591",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Load & Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "09b81ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "toxic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "severe_toxic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "obscene",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "threat",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "insult",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "identity_hate",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "4ea5a0f9-5d9d-4be5-9e8a-01f830a9a7a9",
       "rows": [
        [
         "0",
         "0000997932d777bf",
         "Explanation\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "1",
         "000103f0d9cfb60f",
         "D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "2",
         "000113f07ec002fd",
         "Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "3",
         "0001b41b1c6bb37e",
         "\"\nMore\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\n\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "4",
         "0001d958c54c6e35",
         "You, sir, are my hero. Any chance you remember what page that's on?",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert os.path.exists(DATA_PATH), f\"File not found: {DATA_PATH}\\nMake sure train.csv is available.\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6e2ba073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic            0.095844\n",
      "obscene          0.052948\n",
      "insult           0.049364\n",
      "severe_toxic     0.009996\n",
      "identity_hate    0.008805\n",
      "threat           0.002996\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Basic checks\n",
    "for col in LABELS:\n",
    "    assert col in df.columns, f\"Missing label column '{col}' in train.csv\"\n",
    "\n",
    "df[\"total_labels\"] = df[LABELS].sum(axis=1)\n",
    "print(df[LABELS].mean().sort_values(ascending=False))  # class prevalence\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "df[LABELS].mean().sort_values(ascending=False).plot(kind=\"bar\")\n",
    "plt.title(\"Label Prevalence (mean of 0/1)\"); plt.ylabel(\"Proportion\"); plt.grid(axis='y'); plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "df[\"comment_text_len\"] = df[\"comment_text\"].astype(str).str.len()\n",
    "sns.histplot(df[\"comment_text_len\"], bins=50, kde=False)\n",
    "plt.title(\"Comment length distribution\"); plt.xlabel(\"chars\"); plt.grid(axis='y'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919fa9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db3b3f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quick train: (8000, 10) Quick val: (2000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Standard split: keep a holdout test set from training csv (since Kaggle provides separate test on platform)\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED, stratify=(df[LABELS].sum(axis=1)>0))\n",
    "\n",
    "# Smaller sampled sets for quick benchmark\n",
    "def sample_balanced(df_in, n):\n",
    "    n = min(n, len(df_in))\n",
    "    # simple random for speed; optionally stratify by 'any_toxic'\n",
    "    return df_in.sample(n=n, random_state=SEED)\n",
    "\n",
    "train_quick = sample_balanced(train_df, QUICK_TRAIN_SIZE)\n",
    "val_quick   = sample_balanced(test_df, QUICK_VAL_SIZE)\n",
    "\n",
    "print(\"Quick train:\", train_quick.shape, \"Quick val:\", val_quick.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34454a8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Hugging Face Datasets & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52cc68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def to_hf_dataset(df_in):\n",
    "    # Keep your existing structure: text + multi-hot columns\n",
    "    return Dataset.from_pandas(df_in[[\"comment_text\"] + LABELS].reset_index(drop=True))\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_length):\n",
    "    return tokenizer(\n",
    "        examples[\"comment_text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "def build_hf_splits(train_df, val_df, tokenizer, max_length):\n",
    "    ds_train = to_hf_dataset(train_df)\n",
    "    ds_val   = to_hf_dataset(val_df)\n",
    "\n",
    "    def add_labels_and_tokens(batch):\n",
    "        # 1) tokenize the text\n",
    "        tok = tokenizer(\n",
    "            batch[\"comment_text\"],\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        # 2) build labels as float32 arrays from your multi-hot columns\n",
    "        labs = []\n",
    "        for i in range(len(batch[\"comment_text\"])):\n",
    "            labs.append([float(batch[lbl][i]) for lbl in LABELS])\n",
    "        tok[\"labels\"] = np.asarray(labs, dtype=np.float32)\n",
    "        return tok\n",
    "\n",
    "    # Remove raw columns after mapping to keep only model inputs\n",
    "    keep_cols = {\"comment_text\", *LABELS}\n",
    "    remove_cols_train = [c for c in ds_train.column_names if c in keep_cols]\n",
    "    remove_cols_val   = [c for c in ds_val.column_names if c in keep_cols]\n",
    "\n",
    "    ds_train = ds_train.map(add_labels_and_tokens, batched=True, remove_columns=remove_cols_train)\n",
    "    ds_val   = ds_val.map(add_labels_and_tokens,   batched=True, remove_columns=remove_cols_val)\n",
    "\n",
    "    # torch format for Trainer\n",
    "    cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "    if \"token_type_ids\" in ds_train.column_names:\n",
    "        cols.append(\"token_type_ids\")\n",
    "    ds_train = ds_train.with_format(type=\"torch\", columns=cols)\n",
    "    ds_val   = ds_val.with_format(type=\"torch\",   columns=cols)\n",
    "\n",
    "    return {\"train\": ds_train, \"validation\": ds_val}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01905c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Class Weights (to mitigate imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c0fb3721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': np.float64(0.1289755707099587),\n",
       " 'severe_toxic': np.float64(1.2703210320610998),\n",
       " 'obscene': np.float64(0.23299858125743786),\n",
       " 'threat': np.float64(2.727453980601773),\n",
       " 'insult': np.float64(0.25616971088524937),\n",
       " 'identity_hate': np.float64(1.3840811244844817)}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute positive rate per label and derive inverse-frequency weights\n",
    "pos_rates = train_quick[LABELS].mean().values\n",
    "class_weights = 1.0 / np.clip(pos_rates, 1e-6, None)\n",
    "class_weights = class_weights / class_weights.mean()  # normalize\n",
    "CLASS_WEIGHTS_TENSOR = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "dict(zip(LABELS, class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924d7fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Metrics (Multi-Label: macro-F1, weighted-F1, macro ROC-AUC, per-label F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11650b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics_fn(eval_pred):\n",
    "    logits, y_true = eval_pred\n",
    "    probs = sigmoid(logits)\n",
    "\n",
    "    # Tune a global threshold on the eval set (simple & effective)\n",
    "    candidates = np.linspace(0.05, 0.5, 10)\n",
    "    best_t, best_macro = 0.5, -1.0\n",
    "    for t in candidates:\n",
    "        y_hat = (probs >= t).astype(int)\n",
    "        f1m = f1_score(y_true, y_hat, average=\"macro\", zero_division=0)\n",
    "        if f1m > best_macro:\n",
    "            best_macro, best_t = f1m, t\n",
    "\n",
    "    y_pred = (probs >= best_t).astype(int)\n",
    "\n",
    "    f1_macro   = f1_score(y_true, y_pred, average=\"macro\",   zero_division=0)\n",
    "    f1_weight  = f1_score(y_true, y_pred, average=\"weighted\",zero_division=0)\n",
    "    try:\n",
    "        roc_macro = roc_auc_score(y_true, probs, average=\"macro\")\n",
    "    except Exception:\n",
    "        roc_macro = float(\"nan\")\n",
    "\n",
    "    f1_per = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    out = {\n",
    "        \"f1_macro\": float(f1_macro),\n",
    "        \"f1_weight\": float(f1_weight),\n",
    "        \"roc_auc_macro\": float(roc_macro),\n",
    "        \"best_thr\": float(best_t),\n",
    "    }\n",
    "    for i, name in enumerate(LABELS):\n",
    "        out[f\"f1_{name}\"] = float(f1_per[i])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80ba2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Custom Loss: BCEWithLogits + Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da4f8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, *args, processing_class=None, tokenizer=None, class_weights=None, **kwargs):\n",
    "        if processing_class is None and tokenizer is not None:\n",
    "            processing_class = tokenizer\n",
    "\n",
    "        self._class_weights = class_weights\n",
    "        super().__init__(*args, processing_class=processing_class, **kwargs)\n",
    "        self.processing_class = processing_class\n",
    "        self._bce = None\n",
    "\n",
    "    # optional back-compat shim to avoid deprecation noise\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return getattr(self, \"processing_class\", None)\n",
    "\n",
    "    @tokenizer.setter\n",
    "    def tokenizer(self, value):\n",
    "        self.processing_class = value\n",
    "\n",
    "    def _ensure_loss(self):\n",
    "        if self._bce is not None:\n",
    "            return self._bce\n",
    "        pos_weight = None\n",
    "        if self._class_weights is not None:\n",
    "            pos_weight = self._class_weights\n",
    "            if not torch.is_floating_point(pos_weight):\n",
    "                pos_weight = pos_weight.float()\n",
    "            pos_weight = pos_weight.to(self.model.device)\n",
    "        self._bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        return self._bce\n",
    "\n",
    "    # <-- Accept extra kwargs to stay compatible with Trainer\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # accept both \"labels\" and \"label\"\n",
    "        labels = None\n",
    "        if \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        elif \"label\" in inputs:\n",
    "            labels = inputs.pop(\"label\")\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if labels is None:\n",
    "            # If the model produced a loss itself, use it (rare for multi-label)\n",
    "            maybe = getattr(outputs, \"loss\", None)\n",
    "            if maybe is not None:\n",
    "                loss = maybe\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"No labels found in batch. Expected a tensor under 'labels' (or 'label'). \"\n",
    "                    \"Ensure your dataset map returns that key and dtype float32 for multi-label.\"\n",
    "                )\n",
    "        else:\n",
    "            if labels.dtype != torch.float32:\n",
    "                labels = labels.float()\n",
    "            labels = labels.to(logits.device)\n",
    "            loss = self._ensure_loss()(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746f1ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Phase 1 â€“ Quick Head-to-Head (DistilBERT vs BERT-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "abf0b8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.57.1\n",
      "TrainingArguments from: c:\\Users\\natha\\miniconda3\\envs\\DL_ToxicCommentClassif\\lib\\site-packages\\transformers\\training_args.py\n",
      "Device: cuda | bf16: True | fp16: False | tf32: False\n",
      "\n",
      "===== Benchmarking: distilbert-base-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:00<00:00, 13442.73 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 12952.56 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.333100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.124300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.129700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'eval_loss': 0.11643587052822113, 'eval_f1_macro': 0.14006117345432698, 'eval_f1_weight': 0.1866774023938092, 'eval_roc_auc_macro': 0.7382017309818281, 'eval_best_thr': 0.1, 'eval_f1_toxic': 0.19086386250565354, 'eval_f1_severe_toxic': 0.1935483870967742, 'eval_f1_obscene': 0.19724770642201836, 'eval_f1_threat': 0.019417475728155338, 'eval_f1_insult': 0.1984732824427481, 'eval_f1_identity_hate': 0.04081632653061224, 'eval_runtime': 8.016, 'eval_samples_per_second': 249.501, 'eval_steps_per_second': 15.594, 'epoch': 1.0}\n",
      "\n",
      "===== Benchmarking: bert-base-uncased =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:00<00:00, 11455.59 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 11597.93 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 03:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.512400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.107200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: {'eval_loss': 0.0930660143494606, 'eval_f1_macro': 0.0660505918258851, 'eval_f1_weight': 0.13172871495456806, 'eval_roc_auc_macro': 0.6989373571579294, 'eval_best_thr': 0.05, 'eval_f1_toxic': 0.19086386250565354, 'eval_f1_severe_toxic': 0.015174506828528073, 'eval_f1_obscene': 0.08703969392635102, 'eval_f1_threat': 0.0029955067398901645, 'eval_f1_insult': 0.08337326305701964, 'eval_f1_identity_hate': 0.01685671789786812, 'eval_runtime': 15.8301, 'eval_samples_per_second': 126.342, 'eval_steps_per_second': 7.896, 'epoch': 1.0}\n",
      "\n",
      "Benchmark summary:\n",
      "{\n",
      "  \"distilbert-base-uncased\": {\n",
      "    \"eval_loss\": 0.11643587052822113,\n",
      "    \"eval_f1_macro\": 0.14006117345432698,\n",
      "    \"eval_f1_weight\": 0.1866774023938092,\n",
      "    \"eval_roc_auc_macro\": 0.7382017309818281,\n",
      "    \"eval_best_thr\": 0.1,\n",
      "    \"eval_f1_toxic\": 0.19086386250565354,\n",
      "    \"eval_f1_severe_toxic\": 0.1935483870967742,\n",
      "    \"eval_f1_obscene\": 0.19724770642201836,\n",
      "    \"eval_f1_threat\": 0.019417475728155338,\n",
      "    \"eval_f1_insult\": 0.1984732824427481,\n",
      "    \"eval_f1_identity_hate\": 0.04081632653061224,\n",
      "    \"eval_runtime\": 8.016,\n",
      "    \"eval_samples_per_second\": 249.501,\n",
      "    \"eval_steps_per_second\": 15.594,\n",
      "    \"epoch\": 1.0\n",
      "  },\n",
      "  \"bert-base-uncased\": {\n",
      "    \"eval_loss\": 0.0930660143494606,\n",
      "    \"eval_f1_macro\": 0.0660505918258851,\n",
      "    \"eval_f1_weight\": 0.13172871495456806,\n",
      "    \"eval_roc_auc_macro\": 0.6989373571579294,\n",
      "    \"eval_best_thr\": 0.05,\n",
      "    \"eval_f1_toxic\": 0.19086386250565354,\n",
      "    \"eval_f1_severe_toxic\": 0.015174506828528073,\n",
      "    \"eval_f1_obscene\": 0.08703969392635102,\n",
      "    \"eval_f1_threat\": 0.0029955067398901645,\n",
      "    \"eval_f1_insult\": 0.08337326305701964,\n",
      "    \"eval_f1_identity_hate\": 0.01685671789786812,\n",
      "    \"eval_runtime\": 15.8301,\n",
      "    \"eval_samples_per_second\": 126.342,\n",
      "    \"eval_steps_per_second\": 7.896,\n",
      "    \"epoch\": 1.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os, json, dataclasses, inspect\n",
    "import torch\n",
    "import transformers as hf\n",
    "from packaging import version\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, TrainingArguments\n",
    ")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def supports_tf32() -> bool:\n",
    "    # Require CUDA runtime + Ampere (SM 8.0+) to safely enable TF32\n",
    "    if not torch.cuda.is_available() or torch.version.cuda is None:\n",
    "        return False\n",
    "    major, minor = torch.cuda.get_device_capability()\n",
    "    return major >= 8  # Ampere or newer\n",
    "\n",
    "def make_training_args(**kwargs):\n",
    "    # Keep only the kwargs your installed TrainingArguments supports\n",
    "    valid = {f.name for f in dataclasses.fields(TrainingArguments)}\n",
    "    filtered = {k: v for k, v in kwargs.items() if k in valid and v is not None}\n",
    "    return TrainingArguments(**filtered)\n",
    "\n",
    "# ---------- Device & dtypes ----------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_bf16 = (DEVICE == \"cuda\") and torch.cuda.is_bf16_supported()\n",
    "use_fp16 = (DEVICE == \"cuda\") and (not use_bf16)\n",
    "use_tf32 = supports_tf32()  # <-- SAFE\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    # Only set these hints when CUDA is available\n",
    "    try:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = use_tf32\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Transformers:\", hf.__version__)\n",
    "print(\"TrainingArguments from:\", inspect.getsourcefile(TrainingArguments))\n",
    "print(\"Device:\", DEVICE, \"| bf16:\", use_bf16, \"| fp16:\", use_fp16, \"| tf32:\", use_tf32)\n",
    "\n",
    "# Version-proof eval strategy key (v4 vs v5)\n",
    "eval_kw = (\n",
    "    {\"evaluation_strategy\": \"epoch\"}  # transformers 4.x\n",
    "    if version.parse(hf.__version__) < version.parse(\"5.0.0\")\n",
    "    else {\"eval_strategy\": \"epoch\"}   # transformers 5.x\n",
    ")\n",
    "\n",
    "results_benchmark = {}\n",
    "\n",
    "# (Optional) ensure class weights live on the same device the model will use\n",
    "if DEVICE == \"cuda\" and CLASS_WEIGHTS_TENSOR.device.type != \"cuda\":\n",
    "    CLASS_WEIGHTS_TENSOR = CLASS_WEIGHTS_TENSOR.to(\"cuda\")\n",
    "\n",
    "for model_name, cfg in BASE_MODELS.items():\n",
    "    print(f\"\\n===== Benchmarking: {model_name} =====\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    ds_quick = build_hf_splits(train_quick, val_quick, tokenizer, cfg[\"max_length\"])\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    num_labels = len(LABELS)\n",
    "    torch_dtype = torch.bfloat16 if use_bf16 else (torch.float16 if use_fp16 else torch.float32)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "    model  = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=torch_dtype\n",
    "    )\n",
    "\n",
    "    # Build args with only supported fields; tf32 is passed as False on unsupported hw\n",
    "    import platform\n",
    "\n",
    "    num_workers = 0 if platform.system() == \"Windows\" else max(1, (os.cpu_count() or 4)//2)\n",
    "\n",
    "    args = make_training_args(\n",
    "        output_dir=os.path.join(OUT_DIR, f\"{model_name.replace('/','_')}_bench\"),\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LR_BENCHMARK,\n",
    "        num_train_epochs=EPOCHS_BENCHMARK,\n",
    "\n",
    "        # make v4/v5 happy\n",
    "        **eval_kw,\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=False,\n",
    "        disable_tqdm=False,\n",
    "\n",
    "        # <<< fix the crash >>>\n",
    "        gradient_accumulation_steps=1,   # ensure it's an int, not None\n",
    "\n",
    "        # GPU-friendly flags:\n",
    "        bf16=use_bf16,\n",
    "        fp16=use_fp16,\n",
    "        tf32=use_tf32,\n",
    "        dataloader_pin_memory=(DEVICE == \"cuda\"),\n",
    "        dataloader_num_workers=num_workers,\n",
    "        dataloader_persistent_workers=False,\n",
    "\n",
    "        optim=\"adamw_torch\",\n",
    "        gradient_checkpointing=True,\n",
    "        label_names=[\"labels\"],\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        # torch_compile=True,\n",
    "    )\n",
    "\n",
    "    trainer = MultiLabelTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds_quick[\"train\"],\n",
    "        eval_dataset=ds_quick[\"validation\"],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        class_weights=CLASS_WEIGHTS_TENSOR,\n",
    "    )\n",
    "\n",
    "    train_out = trainer.train()\n",
    "    eval_out  = trainer.evaluate()\n",
    "    results_benchmark[model_name] = eval_out\n",
    "    print(f\"Eval: {eval_out}\")\n",
    "\n",
    "print(\"\\nBenchmark summary:\")\n",
    "print(json.dumps(results_benchmark, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282eda9a",
   "metadata": {},
   "source": [
    "> **Choose the final model**: pick the one with higher `f1_macro` (and `roc_auc_macro`). Typically DistilBERT is faster; BERT-base may be slightly better if you have GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9b894535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Auto-select best by f1_macro\n",
    "best_model_name = max(results_benchmark.keys(), key=lambda k: results_benchmark[k][\"eval_f1_macro\"])\n",
    "best_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bfe5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Phase 2 â€“ Final Training & Optimization (single chosen model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "700353d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final chosen model: distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 129251/129251 [00:10<00:00, 12866.58 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14362/14362 [00:01<00:00, 12119.83 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 129251 Val samples: 14362\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24237' max='24237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24237/24237 1:17:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weight</th>\n",
       "      <th>Roc Auc Macro</th>\n",
       "      <th>Best Thr</th>\n",
       "      <th>F1 Toxic</th>\n",
       "      <th>F1 Severe Toxic</th>\n",
       "      <th>F1 Obscene</th>\n",
       "      <th>F1 Threat</th>\n",
       "      <th>F1 Insult</th>\n",
       "      <th>F1 Identity Hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.054640</td>\n",
       "      <td>0.456249</td>\n",
       "      <td>0.635617</td>\n",
       "      <td>0.961958</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.645346</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.740847</td>\n",
       "      <td>0.093023</td>\n",
       "      <td>0.662172</td>\n",
       "      <td>0.237129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.054487</td>\n",
       "      <td>0.458463</td>\n",
       "      <td>0.646431</td>\n",
       "      <td>0.964240</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.659155</td>\n",
       "      <td>0.345578</td>\n",
       "      <td>0.748603</td>\n",
       "      <td>0.089347</td>\n",
       "      <td>0.678832</td>\n",
       "      <td>0.229261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051800</td>\n",
       "      <td>0.054486</td>\n",
       "      <td>0.458263</td>\n",
       "      <td>0.646987</td>\n",
       "      <td>0.964314</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.661668</td>\n",
       "      <td>0.344640</td>\n",
       "      <td>0.746518</td>\n",
       "      <td>0.089347</td>\n",
       "      <td>0.678832</td>\n",
       "      <td>0.228571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.054486941546201706,\n",
       " 'eval_f1_macro': 0.45846277396578317,\n",
       " 'eval_f1_weight': 0.6464305719930997,\n",
       " 'eval_roc_auc_macro': 0.9642398655311939,\n",
       " 'eval_best_thr': 0.1,\n",
       " 'eval_f1_toxic': 0.6591549295774648,\n",
       " 'eval_f1_severe_toxic': 0.345578231292517,\n",
       " 'eval_f1_obscene': 0.7486033519553073,\n",
       " 'eval_f1_threat': 0.08934707903780069,\n",
       " 'eval_f1_insult': 0.6788321167883211,\n",
       " 'eval_f1_identity_hate': 0.22926093514328807,\n",
       " 'eval_runtime': 57.6606,\n",
       " 'eval_samples_per_second': 249.078,\n",
       " 'eval_steps_per_second': 15.574,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, platform\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers as hf\n",
    "from packaging import version\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# ---------- device & dtype ----------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "use_bf16 = (DEVICE == \"cuda\") and torch.cuda.is_bf16_supported()\n",
    "use_fp16 = (DEVICE == \"cuda\") and (not use_bf16)\n",
    "\n",
    "def supports_tf32() -> bool:\n",
    "    if not torch.cuda.is_available() or torch.version.cuda is None:\n",
    "        return False\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    return major >= 8  # Ampere+\n",
    "\n",
    "use_tf32 = supports_tf32()\n",
    "if DEVICE == \"cuda\":\n",
    "    try: torch.backends.cuda.matmul.allow_tf32 = use_tf32\n",
    "    except Exception: pass\n",
    "    try: torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception: pass\n",
    "\n",
    "# ---------- pick model ----------\n",
    "FINAL_MODEL = best_model_name  # or set manually e.g. \"distilbert-base-uncased\"\n",
    "print(\"Final chosen model:\", FINAL_MODEL)\n",
    "\n",
    "# ---------- split ----------\n",
    "train_full, val_full = train_test_split(\n",
    "    train_df,\n",
    "    test_size=(1 - FINAL_TRAIN_FRACTION),\n",
    "    random_state=SEED,\n",
    "    stratify=(train_df[LABELS].sum(axis=1) > 0)\n",
    ")\n",
    "\n",
    "# ---------- tokenizer / datasets ----------\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL, use_fast=True)\n",
    "max_len   = BASE_MODELS[FINAL_MODEL][\"max_length\"]\n",
    "ds_full   = build_hf_splits(train_full, val_full, tokenizer, max_len)\n",
    "\n",
    "# data collator (ok to keep)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# ---------- model ----------\n",
    "num_labels = len(LABELS)\n",
    "torch_dtype = torch.bfloat16 if use_bf16 else (torch.float16 if use_fp16 else torch.float32)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    FINAL_MODEL,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "model  = AutoModelForSequenceClassification.from_pretrained(\n",
    "    FINAL_MODEL,\n",
    "    config=config,\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(ds_full[\"train\"]), \"Val samples:\", len(ds_full[\"validation\"]))\n",
    "\n",
    "# ---------- TrainingArguments (v4/v5 compatible) ----------\n",
    "num_workers = 0 if platform.system() == \"Windows\" else max(1, (os.cpu_count() or 4)//2)\n",
    "eval_kw = (\n",
    "    {\"evaluation_strategy\": \"epoch\"} if version.parse(hf.__version__) < version.parse(\"5.0.0\")\n",
    "    else {\"eval_strategy\": \"epoch\"}\n",
    ")\n",
    "\n",
    "args_final = make_training_args(\n",
    "    output_dir=os.path.join(OUT_DIR, f\"{FINAL_MODEL.replace('/','_')}_final\"),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR_FINAL,\n",
    "    num_train_epochs=EPOCHS_FINAL,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "\n",
    "    # labels & stability\n",
    "    label_names=[\"labels\"],\n",
    "    gradient_accumulation_steps=1,\n",
    "\n",
    "    # GPU / dataloader flags\n",
    "    bf16=use_bf16,\n",
    "    fp16=use_fp16,\n",
    "    tf32=use_tf32,\n",
    "    dataloader_pin_memory=(DEVICE == \"cuda\"),\n",
    "    dataloader_num_workers=num_workers,\n",
    "    dataloader_persistent_workers=False,\n",
    "\n",
    "    # mild regularization\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "\n",
    "    # evaluation strategy (v4 vs v5) â€” pass BOTH, the helper will keep the valid one\n",
    "    evaluation_strategy=\"epoch\",   # for transformers < 5\n",
    "    eval_strategy=\"epoch\",         # for transformers >= 5\n",
    ")\n",
    "\n",
    "early_stop = EarlyStoppingCallback(\n",
    "    early_stopping_patience=PATIENCE,\n",
    "    early_stopping_threshold=0.0\n",
    ")\n",
    "\n",
    "trainer_final = MultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=args_final,\n",
    "    train_dataset=ds_full[\"train\"],\n",
    "    eval_dataset=ds_full[\"validation\"],\n",
    "    processing_class=tokenizer,      # <-- future-proof (replaces tokenizer=tokenizer)\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_fn,\n",
    "    callbacks=[early_stop],\n",
    "    class_weights=CLASS_WEIGHTS_TENSOR\n",
    ")\n",
    "\n",
    "train_res = trainer_final.train()\n",
    "best_metrics = trainer_final.evaluate()\n",
    "best_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4521b4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 9. Detailed Evaluation: Per-Label Reports, Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "41edb674",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Column 'toxic' doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m logits \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[0;32m      4\u001b[0m probs  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mlogits))\n\u001b[1;32m----> 5\u001b[0m y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([ds_full[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m][l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m LABELS], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (probs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMacro F1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, classification_report(y_true, y_pred, target_names\u001b[38;5;241m=\u001b[39mLABELS, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "Cell \u001b[1;32mIn[104], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m logits \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mpredictions\n\u001b[0;32m      4\u001b[0m probs  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mlogits))\n\u001b[1;32m----> 5\u001b[0m y_true \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([\u001b[43mds_full\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m LABELS], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (probs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMacro F1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, classification_report(y_true, y_pred, target_names\u001b[38;5;241m=\u001b[39mLABELS, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\natha\\miniconda3\\envs\\DL_ToxicCommentClassif\\lib\\site-packages\\datasets\\arrow_dataset.py:2866\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   2865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrow\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolars\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 2866\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(key)\n",
      "File \u001b[1;32mc:\\Users\\natha\\miniconda3\\envs\\DL_ToxicCommentClassif\\lib\\site-packages\\datasets\\arrow_dataset.py:658\u001b[0m, in \u001b[0;36mColumn.__init__\u001b[1;34m(self, source, column_name)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumn_name \u001b[38;5;241m=\u001b[39m column_name\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m source\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[1;32m--> 658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mfeatures[column_name]\n",
      "\u001b[1;31mValueError\u001b[0m: Column 'toxic' doesn't exist."
     ]
    }
   ],
   "source": [
    "# Predictions on validation set\n",
    "raw = trainer_final.predict(ds_full[\"validation\"])\n",
    "logits = raw.predictions\n",
    "probs  = 1 / (1 + np.exp(-logits))\n",
    "y_true = np.stack([ds_full[\"validation\"][l] for l in LABELS], axis=1)\n",
    "y_pred = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"Macro F1:\", classification_report(y_true, y_pred, target_names=LABELS, zero_division=0))\n",
    "\n",
    "# ROC-AUC per label\n",
    "try:\n",
    "    roc_per_label = {LABELS[i]: roc_auc_score(y_true[:,i], probs[:,i]) for i in range(len(LABELS))}\n",
    "    pd.Series(roc_per_label).sort_values(ascending=False)\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC per label error:\", e)\n",
    "\n",
    "# Plot probability histograms per label\n",
    "fig, axes = plt.subplots(2,3, figsize=(14,8))\n",
    "axes = axes.ravel()\n",
    "for i, lab in enumerate(LABELS):\n",
    "    sns.histplot(probs[:,i], bins=30, ax=axes[i], color=\"#3A8BFF\")\n",
    "    axes[i].set_title(f\"Predicted P({lab})\")\n",
    "    axes[i].grid(axis='y')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc4196",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Save Artifacts & Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = os.path.join(OUT_DIR, f\"{FINAL_MODEL.replace('/','_')}_BEST\")\n",
    "trainer_final.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "def predict_texts(texts, path=SAVE_DIR, threshold=0.5):\n",
    "    tok = AutoTokenizer.from_pretrained(path)\n",
    "    cfg = AutoConfig.from_pretrained(path)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(path).to(DEVICE)\n",
    "    mdl.eval()\n",
    "    enc = tok(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = mdl(**enc).logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    return probs, preds\n",
    "\n",
    "probs, preds = predict_texts([\"I will find you and I will hurt you.\",\"Have a wonderful day!\"])\n",
    "pd.DataFrame(probs, columns=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c23c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10B. Evaluate on Kaggle Test Set & Create Submission (Optional)\n",
    "\n",
    "Use the official Kaggle `test.csv` and `test_labels.csv` to:\n",
    "1) Evaluate the model locally on the subset where labels are released (labels â‰  -1), and  \n",
    "2) Produce a `submission.csv` for Kaggle (probabilities only).\n",
    "\n",
    "> Place `test.csv`, `test_labels.csv`, and `sample_submission.csv` in the project folder (same as `train.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ecca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, roc_auc_score, classification_report\n",
    "\n",
    "# Paths (adjust if needed)\n",
    "TEST_PATH          = \"./test.csv\"\n",
    "TEST_LABELS_PATH   = \"./test_labels.csv\"\n",
    "SAMPLE_SUB_PATH    = \"./sample_submission.csv\"\n",
    "SUBMISSION_OUTPATH = \"./submission.csv\"\n",
    "\n",
    "# 1) Load Kaggle test and labels\n",
    "test_df        = pd.read_csv(TEST_PATH)\n",
    "test_labels_df = pd.read_csv(TEST_LABELS_PATH)\n",
    "\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"Test labels shape:\", test_labels_df.shape)\n",
    "display(test_df.head())\n",
    "display(test_labels_df.head())\n",
    "\n",
    "# 2) Generate probabilities for ALL test comments\n",
    "test_texts = test_df[\"comment_text\"].astype(str).tolist()\n",
    "probs_all, preds_all = predict_texts(test_texts, path=SAVE_DIR, threshold=0.5)  # uses the saved best model\n",
    "print(\"Predictions shape:\", probs_all.shape)\n",
    "\n",
    "# 3) Evaluate only where Kaggle released ground-truth labels (rows with labels != -1)\n",
    "mask_valid = (test_labels_df[\"toxic\"] != -1)\n",
    "valid_idx = test_labels_df.index[mask_valid].tolist()\n",
    "\n",
    "y_true = test_labels_df.loc[valid_idx, LABELS].values.astype(int)\n",
    "y_prob = probs_all[valid_idx]\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "# Metrics on released subset\n",
    "f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_weight  = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "try:\n",
    "    roc_macro = roc_auc_score(y_true, y_prob, average=\"macro\")\n",
    "except Exception:\n",
    "    roc_macro = float(\"nan\")\n",
    "\n",
    "print(f\"Released-subset metrics | F1_macro: {f1_macro:.4f} | F1_weighted: {f1_weight:.4f} | ROC_AUC_macro: {roc_macro:.4f}\")\n",
    "print(\"\\nPer-label F1:\")\n",
    "print(pd.Series(\n",
    "    f1_score(y_true, y_pred, average=None, zero_division=0),\n",
    "    index=LABELS\n",
    ").sort_values(ascending=False))\n",
    "\n",
    "# Optional: detailed report\n",
    "print(\"\\nClassification report (threshold = 0.5):\")\n",
    "print(classification_report(y_true, y_pred, target_names=LABELS, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b0e24",
   "metadata": {},
   "source": [
    "### (Optional) Threshold Tuning on Validation Set\n",
    "Optimize a single global threshold using your **validation** predictions to maximize macro-F1, then reuse it on the Kaggle test subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47258743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse validation predictions from section 9:\n",
    "#   raw = trainer_final.predict(ds_full[\"validation\"])\n",
    "# If you no longer have `raw`, re-run that cell first.\n",
    "val_logits = raw.predictions\n",
    "val_probs  = 1 / (1 + np.exp(-val_logits))\n",
    "val_true   = np.stack([ds_full[\"validation\"][l] for l in LABELS], axis=1)\n",
    "\n",
    "best_thr, best_f1 = 0.5, -1\n",
    "for thr in np.linspace(0.2, 0.8, 13):\n",
    "    f1m = f1_score(val_true, (val_probs >= thr).astype(int), average=\"macro\", zero_division=0)\n",
    "    if f1m > best_f1:\n",
    "        best_f1, best_thr = f1m, float(thr)\n",
    "\n",
    "print(f\"Best global threshold on validation: {best_thr:.2f} (macro-F1={best_f1:.4f})\")\n",
    "\n",
    "# Apply tuned threshold on Kaggle test subset and re-evaluate\n",
    "y_pred_tuned = (y_prob >= best_thr).astype(int)\n",
    "f1_macro_tuned  = f1_score(y_true, y_pred_tuned, average=\"macro\", zero_division=0)\n",
    "f1_weight_tuned = f1_score(y_true, y_pred_tuned, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(f\"Released-subset metrics (tuned thr {best_thr:.2f}) | F1_macro: {f1_macro_tuned:.4f} | F1_weighted: {f1_weight_tuned:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb935a3e",
   "metadata": {},
   "source": [
    "### Create `submission.csv` (for Kaggle)\n",
    "Fill the sample submission with your **probabilities** (not hard labels) and save.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4437c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "assert all(col in sub.columns for col in LABELS), \"Sample submission missing expected label columns.\"\n",
    "\n",
    "# IMPORTANT: order must match test_df rows\n",
    "# sub['id'] aligns with test_df['id'] by Kaggle convention\n",
    "sub[LABELS] = probs_all  # use probabilities (floats in [0,1])\n",
    "sub.to_csv(SUBMISSION_OUTPATH, index=False)\n",
    "print(\"Saved Kaggle submission to:\", os.path.abspath(SUBMISSION_OUTPATH))\n",
    "\n",
    "# Tip: upload this CSV on the Kaggle competition page to get a leaderboard score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36eaa42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Gradio Demo (Local App)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def classify_comment(text, threshold=0.5):\n",
    "    pr, pd_bin = predict_texts([text], path=SAVE_DIR, threshold=float(threshold))\n",
    "    pr = pr[0]; pd_bin = pd_bin[0]\n",
    "    result = {LABELS[i]: float(pr[i]) for i in range(len(LABELS))}\n",
    "    preds  = {LABELS[i]: int(pd_bin[i]) for i in range(len(LABELS))}\n",
    "    return result, preds\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=classify_comment,\n",
    "    inputs=[gr.Textbox(lines=4, label=\"Comment\"), gr.Slider(0.1, 0.9, value=0.5, step=0.05, label=\"Decision threshold\")],\n",
    "    outputs=[gr.Label(num_top_classes=6, label=\"Probabilities\"), gr.JSON(label=\"Binary predictions (â‰¥ threshold)\")],\n",
    "    title=\"Jigsaw Toxic Comment Classifier\",\n",
    "    description=\"DistilBERT/BERT multi-label classifier with sigmoid outputs.\"\n",
    ")\n",
    "\n",
    "# Uncomment to launch locally\n",
    "# demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03271bd9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Report Pointers (for exam write-up)\n",
    "\n",
    "- **Problem framing**: Online moderation, multi-label toxicity detection.  \n",
    "- **Data**: Jigsaw (2018), size, class imbalance, preprocessing decisions.  \n",
    "- **Method**: Phase 1 benchmark (DistilBERT vs BERT-base) â†’ Phase 2 optimization (chosen model).  \n",
    "- **Loss**: `BCEWithLogitsLoss` with per-label `pos_weight`.  \n",
    "- **Optimization**: LR 2e-5, batch 16 (GPU), early stopping, weight decay.  \n",
    "- **Metrics**: Macro-F1 primary, Weighted-F1 secondary, Macro ROC-AUC; per-label F1 table.  \n",
    "- **Results**: Show curves, tables, sample predictions.  \n",
    "- **Ethics**: Bias, fairness, explainability (optional: SHAP on token importance), threshold choice & moderation policy.  \n",
    "- **Reproducibility**: random seeds, environment, versions, saved artifacts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdeb64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. (Optional) Tips if CPU-only\n",
    "\n",
    "- Use **DistilBERT** only; set `QUICK_TRAIN_SIZE=1000`, `QUICK_VAL_SIZE=400`, `EPOCHS_FINAL=2`.  \n",
    "- Reduce `max_length` to **128**.  \n",
    "- Consider **gradient accumulation** to emulate larger batch sizes:\n",
    "  - Add `gradient_accumulation_steps=2` in `TrainingArguments`.\n",
    "- Expect much slower training; use the notebook to validate pipeline, then scale on Colab GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_ToxicCommentClassif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
