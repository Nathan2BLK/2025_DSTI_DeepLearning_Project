{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882af774",
   "metadata": {},
   "source": [
    "# ðŸ§ª Toxic Comment Classification (Jigsaw) â€” End-to-End Notebook\n",
    "# Single-model optimization with a quick 2-model benchmark phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a43e2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Setup & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94182417",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/transformers/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/datasets/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/datasets/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/datasets/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/datasets/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/datasets/\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/accelerate/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/accelerate/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/accelerate/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/accelerate/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(\"Can't connect to HTTPS URL because the SSL module is not available.\")': /simple/accelerate/\n",
      "ERROR: Could not find a version that satisfies the requirement accelerate (from versions: none)\n",
      "ERROR: No matching distribution found for accelerate\n",
      "WARNING: pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.\n"
     ]
    }
   ],
   "source": [
    "!pip -q install transformers datasets accelerate evaluate scikit-learn matplotlib seaborn gradio torchmetrics --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, math, json, random, gc, time, shutil\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "import transformers\n",
    "\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED   = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ====== PATHS: set this to your local location ======\n",
    "DATA_PATH = \"./data/train.csv\"  # <- change if needed\n",
    "OUT_DIR   = \"./outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Labels in Jigsaw dataset\n",
    "LABELS = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "\n",
    "# Base configs\n",
    "BASE_MODELS = {\n",
    "    \"distilbert-base-uncased\": {\"max_length\": 192},\n",
    "    \"bert-base-uncased\":      {\"max_length\": 192}\n",
    "}\n",
    "\n",
    "# Subset sizes (smaller on CPU to keep it practical)\n",
    "if DEVICE == \"cuda\":\n",
    "    QUICK_TRAIN_SIZE = 8000   # for quick benchmark per model\n",
    "    QUICK_VAL_SIZE   = 2000\n",
    "else:\n",
    "    QUICK_TRAIN_SIZE = 2000\n",
    "    QUICK_VAL_SIZE   = 800\n",
    "\n",
    "# Final training sizes (Phase 2) â€” you can increase if GPU available\n",
    "FINAL_TRAIN_FRACTION = 0.9   # 90% of train split used for training; rest for val\n",
    "EPOCHS_BENCHMARK     = 1     # quick head-to-head\n",
    "EPOCHS_FINAL         = 3     # main optimization run\n",
    "BATCH_SIZE           = 16 if DEVICE == \"cuda\" else 8\n",
    "LR_BENCHMARK         = 2e-5\n",
    "LR_FINAL             = 2e-5\n",
    "WEIGHT_DECAY         = 0.01\n",
    "PATIENCE             = 2     # early stopping patience for final run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d6591",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Load & Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b81ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(DATA_PATH), f\"File not found: {DATA_PATH}\\nMake sure train.csv is available.\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ba073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic checks\n",
    "for col in LABELS:\n",
    "    assert col in df.columns, f\"Missing label column '{col}' in train.csv\"\n",
    "\n",
    "df[\"total_labels\"] = df[LABELS].sum(axis=1)\n",
    "print(df[LABELS].mean().sort_values(ascending=False))  # class prevalence\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "df[LABELS].mean().sort_values(ascending=False).plot(kind=\"bar\")\n",
    "plt.title(\"Label Prevalence (mean of 0/1)\"); plt.ylabel(\"Proportion\"); plt.grid(axis='y'); plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "df[\"comment_text_len\"] = df[\"comment_text\"].astype(str).str.len()\n",
    "sns.histplot(df[\"comment_text_len\"], bins=50, kde=False)\n",
    "plt.title(\"Comment length distribution\"); plt.xlabel(\"chars\"); plt.grid(axis='y'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919fa9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b3f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard split: keep a holdout test set from training csv (since Kaggle provides separate test on platform)\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED, stratify=(df[LABELS].sum(axis=1)>0))\n",
    "\n",
    "# Smaller sampled sets for quick benchmark\n",
    "def sample_balanced(df_in, n):\n",
    "    n = min(n, len(df_in))\n",
    "    # simple random for speed; optionally stratify by 'any_toxic'\n",
    "    return df_in.sample(n=n, random_state=SEED)\n",
    "\n",
    "train_quick = sample_balanced(train_df, QUICK_TRAIN_SIZE)\n",
    "val_quick   = sample_balanced(test_df, QUICK_VAL_SIZE)\n",
    "\n",
    "print(\"Quick train:\", train_quick.shape, \"Quick val:\", val_quick.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34454a8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Hugging Face Datasets & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cc68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_hf_dataset(df_in):\n",
    "    return Dataset.from_pandas(df_in[[\"comment_text\"] + LABELS].reset_index(drop=True))\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_length):\n",
    "    return tokenizer(\n",
    "        examples[\"comment_text\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "def build_hf_splits(train_df, val_df, tokenizer, max_length):\n",
    "    ds_train = to_hf_dataset(train_df)\n",
    "    ds_val   = to_hf_dataset(val_df)\n",
    "\n",
    "    ds_train = ds_train.map(lambda x: tokenize_function(x, tokenizer, max_length), batched=True, remove_columns=[\"comment_text\"])\n",
    "    ds_val   = ds_val.map(lambda x: tokenize_function(x, tokenizer, max_length),   batched=True, remove_columns=[\"comment_text\"])\n",
    "    ds = DatasetDict({\"train\": ds_train, \"validation\": ds_val})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01905c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Class Weights (to mitigate imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute positive rate per label and derive inverse-frequency weights\n",
    "pos_rates = train_quick[LABELS].mean().values\n",
    "class_weights = 1.0 / np.clip(pos_rates, 1e-6, None)\n",
    "class_weights = class_weights / class_weights.mean()  # normalize\n",
    "CLASS_WEIGHTS_TENSOR = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "dict(zip(LABELS, class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924d7fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Metrics (Multi-Label: macro-F1, weighted-F1, macro ROC-AUC, per-label F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11650b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_f1 = evaluate.load(\"f1\")\n",
    "metric_accuracy = evaluate.load(\"accuracy\")  # not very meaningful for multi-label, but we can compute thresholded exact-match accuracy\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics_fn(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = sigmoid(logits)\n",
    "    y_true = labels\n",
    "    y_pred = (probs >= 0.5).astype(int)\n",
    "\n",
    "    # Macro/weighted F1 across labels\n",
    "    f1_macro   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_weight  = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "    # ROC-AUC macro\n",
    "    try:\n",
    "        roc_macro = roc_auc_score(y_true, probs, average=\"macro\")\n",
    "    except Exception:\n",
    "        roc_macro = float(\"nan\")\n",
    "    # per-label F1\n",
    "    f1_per_label = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    return {\n",
    "        \"f1_macro\":   f1_macro,\n",
    "        \"f1_weight\":  f1_weight,\n",
    "        \"roc_auc_macro\": roc_macro,\n",
    "        **{f\"f1_{LABELS[i]}\": f1_per_label[i] for i in range(len(LABELS))}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80ba2f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Custom Loss: BCEWithLogits + Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f8f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = torch.stack([inputs.pop(l) for l in LABELS], dim=1).float().to(model.device)\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.logits\n",
    "        # BCEWithLogits with per-label weights\n",
    "        loss_fct = nn.BCEWithLogitsLoss(pos_weight=self.class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746f1ea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Phase 1 â€“ Quick Head-to-Head (DistilBERT vs BERT-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_benchmark = {}\n",
    "\n",
    "for model_name, cfg in BASE_MODELS.items():\n",
    "    print(f\"\\n===== Benchmarking: {model_name} =====\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    ds_quick = build_hf_splits(train_quick, val_quick, tokenizer, cfg[\"max_length\"])\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    num_labels = len(LABELS)\n",
    "    config = AutoConfig.from_pretrained(model_name, num_labels=num_labels, problem_type=\"multi_label_classification\")\n",
    "    model  = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(OUT_DIR, f\"{model_name.replace('/','_')}_bench\"),\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LR_BENCHMARK,\n",
    "        num_train_epochs=EPOCHS_BENCHMARK,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=50,\n",
    "        fp16=(DEVICE==\"cuda\"),\n",
    "        report_to=\"none\",\n",
    "        load_best_model_at_end=False,\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "\n",
    "    trainer = MultiLabelTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds_quick[\"train\"],\n",
    "        eval_dataset=ds_quick[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        class_weights=CLASS_WEIGHTS_TENSOR\n",
    "    )\n",
    "    train_out = trainer.train()\n",
    "    eval_out  = trainer.evaluate()\n",
    "    results_benchmark[model_name] = eval_out\n",
    "    print(f\"Eval: {eval_out}\")\n",
    "\n",
    "print(\"\\nBenchmark summary:\")\n",
    "print(json.dumps(results_benchmark, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282eda9a",
   "metadata": {},
   "source": [
    "> **Choose the final model**: pick the one with higher `f1_macro` (and `roc_auc_macro`). Typically DistilBERT is faster; BERT-base may be slightly better if you have GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b894535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-select best by f1_macro\n",
    "best_model_name = max(results_benchmark.keys(), key=lambda k: results_benchmark[k][\"eval_f1_macro\"])\n",
    "best_model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89bfe5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Phase 2 â€“ Final Training & Optimization (single chosen model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700353d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL = best_model_name  # or set manually: \"distilbert-base-uncased\"\n",
    "print(\"Final chosen model:\", FINAL_MODEL)\n",
    "\n",
    "# Build a larger split from original train_df/test_df\n",
    "train_full, val_full = train_test_split(\n",
    "    train_df, test_size=(1 - FINAL_TRAIN_FRACTION), random_state=SEED,\n",
    "    stratify=(train_df[LABELS].sum(axis=1)>0)\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINAL_MODEL)\n",
    "max_len   = BASE_MODELS[FINAL_MODEL][\"max_length\"]\n",
    "ds_full   = build_hf_splits(train_full, val_full, tokenizer, max_len)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "num_labels = len(LABELS)\n",
    "config = AutoConfig.from_pretrained(FINAL_MODEL, num_labels=num_labels, problem_type=\"multi_label_classification\")\n",
    "model  = AutoModelForSequenceClassification.from_pretrained(FINAL_MODEL, config=config)\n",
    "\n",
    "total_train_steps = (len(ds_full[\"train\"]) // BATCH_SIZE) * EPOCHS_FINAL\n",
    "print(\"Train samples:\", len(ds_full[\"train\"]), \"Val samples:\", len(ds_full[\"validation\"]))\n",
    "\n",
    "args_final = TrainingArguments(\n",
    "    output_dir=os.path.join(OUT_DIR, f\"{FINAL_MODEL.replace('/','_')}_final\"),\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LR_FINAL,\n",
    "    num_train_epochs=EPOCHS_FINAL,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    fp16=(DEVICE==\"cuda\"),\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "early_stop = EarlyStoppingCallback(early_stopping_patience=PATIENCE, early_stopping_threshold=0.0)\n",
    "\n",
    "trainer_final = MultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=args_final,\n",
    "    train_dataset=ds_full[\"train\"],\n",
    "    eval_dataset=ds_full[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_fn,\n",
    "    callbacks=[early_stop],\n",
    "    class_weights=CLASS_WEIGHTS_TENSOR\n",
    ")\n",
    "\n",
    "train_res = trainer_final.train()\n",
    "best_metrics = trainer_final.evaluate()\n",
    "best_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4521b4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 9. Detailed Evaluation: Per-Label Reports, Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41edb674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on validation set\n",
    "raw = trainer_final.predict(ds_full[\"validation\"])\n",
    "logits = raw.predictions\n",
    "probs  = 1 / (1 + np.exp(-logits))\n",
    "y_true = np.stack([ds_full[\"validation\"][l] for l in LABELS], axis=1)\n",
    "y_pred = (probs >= 0.5).astype(int)\n",
    "\n",
    "print(\"Macro F1:\", classification_report(y_true, y_pred, target_names=LABELS, zero_division=0))\n",
    "\n",
    "# ROC-AUC per label\n",
    "try:\n",
    "    roc_per_label = {LABELS[i]: roc_auc_score(y_true[:,i], probs[:,i]) for i in range(len(LABELS))}\n",
    "    pd.Series(roc_per_label).sort_values(ascending=False)\n",
    "except Exception as e:\n",
    "    print(\"ROC-AUC per label error:\", e)\n",
    "\n",
    "# Plot probability histograms per label\n",
    "fig, axes = plt.subplots(2,3, figsize=(14,8))\n",
    "axes = axes.ravel()\n",
    "for i, lab in enumerate(LABELS):\n",
    "    sns.histplot(probs[:,i], bins=30, ax=axes[i], color=\"#3A8BFF\")\n",
    "    axes[i].set_title(f\"Predicted P({lab})\")\n",
    "    axes[i].grid(axis='y')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc4196",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Save Artifacts & Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = os.path.join(OUT_DIR, f\"{FINAL_MODEL.replace('/','_')}_BEST\")\n",
    "trainer_final.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "def predict_texts(texts, path=SAVE_DIR, threshold=0.5):\n",
    "    tok = AutoTokenizer.from_pretrained(path)\n",
    "    cfg = AutoConfig.from_pretrained(path)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(path).to(DEVICE)\n",
    "    mdl.eval()\n",
    "    enc = tok(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = mdl(**enc).logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    return probs, preds\n",
    "\n",
    "probs, preds = predict_texts([\"I will find you and I will hurt you.\",\"Have a wonderful day!\"])\n",
    "pd.DataFrame(probs, columns=LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36eaa42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Gradio Demo (Local App)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def classify_comment(text, threshold=0.5):\n",
    "    pr, pd_bin = predict_texts([text], path=SAVE_DIR, threshold=float(threshold))\n",
    "    pr = pr[0]; pd_bin = pd_bin[0]\n",
    "    result = {LABELS[i]: float(pr[i]) for i in range(len(LABELS))}\n",
    "    preds  = {LABELS[i]: int(pd_bin[i]) for i in range(len(LABELS))}\n",
    "    return result, preds\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=classify_comment,\n",
    "    inputs=[gr.Textbox(lines=4, label=\"Comment\"), gr.Slider(0.1, 0.9, value=0.5, step=0.05, label=\"Decision threshold\")],\n",
    "    outputs=[gr.Label(num_top_classes=6, label=\"Probabilities\"), gr.JSON(label=\"Binary predictions (â‰¥ threshold)\")],\n",
    "    title=\"Jigsaw Toxic Comment Classifier\",\n",
    "    description=\"DistilBERT/BERT multi-label classifier with sigmoid outputs.\"\n",
    ")\n",
    "\n",
    "# Uncomment to launch locally\n",
    "# demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03271bd9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Report Pointers (for exam write-up)\n",
    "\n",
    "- **Problem framing**: Online moderation, multi-label toxicity detection.  \n",
    "- **Data**: Jigsaw (2018), size, class imbalance, preprocessing decisions.  \n",
    "- **Method**: Phase 1 benchmark (DistilBERT vs BERT-base) â†’ Phase 2 optimization (chosen model).  \n",
    "- **Loss**: `BCEWithLogitsLoss` with per-label `pos_weight`.  \n",
    "- **Optimization**: LR 2e-5, batch 16 (GPU), early stopping, weight decay.  \n",
    "- **Metrics**: Macro-F1 primary, Weighted-F1 secondary, Macro ROC-AUC; per-label F1 table.  \n",
    "- **Results**: Show curves, tables, sample predictions.  \n",
    "- **Ethics**: Bias, fairness, explainability (optional: SHAP on token importance), threshold choice & moderation policy.  \n",
    "- **Reproducibility**: random seeds, environment, versions, saved artifacts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdeb64",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. (Optional) Tips if CPU-only\n",
    "\n",
    "- Use **DistilBERT** only; set `QUICK_TRAIN_SIZE=1000`, `QUICK_VAL_SIZE=400`, `EPOCHS_FINAL=2`.  \n",
    "- Reduce `max_length` to **128**.  \n",
    "- Consider **gradient accumulation** to emulate larger batch sizes:\n",
    "  - Add `gradient_accumulation_steps=2` in `TrainingArguments`.\n",
    "- Expect much slower training; use the notebook to validate pipeline, then scale on Colab GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
